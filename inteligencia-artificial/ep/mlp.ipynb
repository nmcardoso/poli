{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EP Inteligência Artificial\n",
    "## Aluno: Natanael Magalhães Cardoso, 8914122"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Itens 1 e 2\n",
    "\n",
    "* Construir uma rede neural do tipo MLP com número configurável de camadas escondidas que seja treinável pelo algoritmo SGD (3.0)\n",
    "  * A única diferença dessa entrega e o trabalho realizado no laboratório é o \"S\" do SGD, que envolve o treinamento utilizando:\n",
    "  * Grupos de exemplos a cada iteração, ao invés do dataset de treino inteiro. Esses grupos são tipicamente chamados de \"batches\".\n",
    "  * A learning rate inicial deve ser reduzida a cada passo para auxiliar a convergência. Formalmente ela deve ser quadraticamente somável.\n",
    "  * Para lr_0 < 1.0, a expressão de decrescimento lr_i = lr_{i-1}/(1+i) é suficiente.\n",
    "* Adicionar um termo de regularização do tipo L2 ao treinamento (2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCS3438 - Inteligência Artificial - 2023/2\n",
    "# Template para aula de laboratório em Redes Neurais - 20/09/2023\n",
    "\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "\n",
    "def sigmoid(x: np.ndarray):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x: np.ndarray):\n",
    "  return x * (1 - x)\n",
    "\n",
    "\n",
    "def mse_loss(y: np.ndarray, y_hat: np.ndarray):\n",
    "  return np.mean(np.power(y - y_hat, 2))\n",
    "\n",
    "\n",
    "def mse_loss_derivative(y: np.ndarray, y_hat: np.ndarray):\n",
    "  return y_hat - y\n",
    "\n",
    "\n",
    "\n",
    "class Layer:\n",
    "  def __init__(self, input_dim: int, output_dim: int, reg_strength: float = 0.0):\n",
    "    self.weights = 2 * np.random.random((input_dim, output_dim)) - 1\n",
    "    self.biases = np.zeros((1, output_dim))\n",
    "    self.input: np.ndarray | None = None\n",
    "    self.output: np.ndarray | None = None\n",
    "    self.reg_strength = reg_strength  # Força da regularização L2\n",
    "\n",
    "  def forward(self, input_data) -> np.ndarray:\n",
    "    self.input = input_data\n",
    "    raw_output = np.dot(input_data, self.weights) + self.biases\n",
    "    self.output = sigmoid(raw_output)\n",
    "    return self.output\n",
    "\n",
    "  def backward(self, output_error: np.ndarray, learning_rate: float) -> np.ndarray:\n",
    "    local_gradient = sigmoid_derivative(self.output)\n",
    "    layer_error = output_error * local_gradient\n",
    "\n",
    "    # Termo de regularização L2\n",
    "    reg_term = 2 * self.reg_strength * self.weights\n",
    "\n",
    "    # Atualiza os pesos e biases usando gradiente descendente com termo de regularização L2\n",
    "    self.weights -= (np.dot(self.input.T, layer_error) + reg_term) * learning_rate\n",
    "    self.biases -= np.sum(layer_error, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    # Retorna o erro para a camada anterior\n",
    "    return np.dot(layer_error, self.weights.T)\n",
    "\n",
    "\n",
    "def forward(input: np.ndarray, layers: list[Layer]):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    input (np.ndarray): Input data\n",
    "    layers (list[Layer]): List of layers\n",
    "\n",
    "  Returns:\n",
    "    np.ndarray: Output of the MLP model\n",
    "  \"\"\"\n",
    "  current_input = input\n",
    "  for layer in layers:\n",
    "    current_input = layer.forward(current_input)\n",
    "  return current_input\n",
    "\n",
    "\n",
    "def backward(\n",
    "  y: np.ndarray, y_hat: np.ndarray, layers: list[Layer], learning_rate: float\n",
    ") -> None:\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    y (np.ndarray): Ground truth\n",
    "    y_hat (np.ndarray): Predicted values\n",
    "    layers (list[Layer]): List of layers\n",
    "    learning_rate (float): Learning rate\n",
    "  \"\"\"\n",
    "  output_error = mse_loss_derivative(y, y_hat)\n",
    "  for layer in reversed(layers):\n",
    "    output_error = layer.backward(output_error, learning_rate)\n",
    "    \n",
    "\n",
    "def train_sgd(\n",
    "  layers: List[Layer], \n",
    "  X: np.ndarray, \n",
    "  y: np.ndarray, \n",
    "  epochs: int, \n",
    "  lr: float, \n",
    "  batch_size: int, \n",
    "  reg_strength: float,\n",
    "  verbose: bool = True,\n",
    "):\n",
    "  # Treinar o modelo MLP\n",
    "  for epoch in range(epochs):\n",
    "    # Embaralhamento SGD\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X_train_shuffled = X[indices]\n",
    "    y_train_shuffled = y[indices]\n",
    "\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "      # Criação dos batches\n",
    "      X_batch = X_train_shuffled[i:i+batch_size]\n",
    "      y_batch = y_train_shuffled[i:i+batch_size]\n",
    "\n",
    "      # Passo forward\n",
    "      y_hat = forward(X_batch, layers)\n",
    "\n",
    "      # Loss com o termo de regularização\n",
    "      loss = mse_loss(y_batch.reshape(-1, 1), y_hat) + 0.5 * reg_strength * sum(np.sum(layer.weights**2) for layer in layers)\n",
    "\n",
    "      # Backward\n",
    "      backward(y_batch.reshape(-1, 1), y_hat, layers, lr)\n",
    "\n",
    "    # Update learning rate\n",
    "    lr = lr / (1 + epoch)\n",
    "\n",
    "    if verbose and epoch % 1000 == 0:\n",
    "      print(f\"Epoch {epoch} Loss: {np.mean(loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 3\n",
    "\n",
    "* Obter o dataset Breast Cancer Winsconsin Dataset (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) (3.0)\n",
    "  * Realizar a divisão do dataset em subset de treino (80%) e de teste (20%)\n",
    "  * Realizar a normalização das features (Z-Score ou Max-min)\n",
    "  * Realizar o treinamento do MLP desenvolvido para classificação do dataset de teste (já com a regularização)\n",
    "  * Reportar a acurácia da classificação\n",
    "  * Reportar a matriz de confusão da classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.19279935262281278\n",
      "Epoch 1000 Loss: 0.07605301964698802\n",
      "Epoch 2000 Loss: 0.08916311712198355\n",
      "Epoch 3000 Loss: 0.12029573452909006\n",
      "Epoch 4000 Loss: 0.07381216851748475\n",
      "Epoch 5000 Loss: 0.08865671909001557\n",
      "Epoch 6000 Loss: 0.17089502855718594\n",
      "Epoch 7000 Loss: 0.11202978619948985\n",
      "Epoch 8000 Loss: 0.13263213383350625\n",
      "Epoch 9000 Loss: 0.10019762431368218\n",
      "\n",
      "Acurácia: 0.9558\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[35  3]\n",
      " [ 2 73]]\n",
      "Verdadeiro Negativo: 35\n",
      "Verdadeito Positivo: 73\n",
      "Falso Positivo: 3\n",
      "Falso Negativo: 2\n"
     ]
    }
   ],
   "source": [
    "def standard_scaler(train_data, test_data=None):\n",
    "  # Calcular a média e o desvio padrão do conjunto de treino\n",
    "  mean = np.mean(train_data, axis=0)\n",
    "  std = np.std(train_data, axis=0)\n",
    "\n",
    "  # Aplicar a normalização Z-Score ao conjunto de treino\n",
    "  train_data_normalized = (train_data - mean) / std\n",
    "\n",
    "  if test_data is not None:\n",
    "    # Aplicar a mesma normalização ao conjunto de teste\n",
    "    test_data_normalized = (test_data - mean) / std\n",
    "    return train_data_normalized, test_data_normalized\n",
    "  else:\n",
    "    return train_data_normalized\n",
    "  \n",
    "  \n",
    "def train_test_split(\n",
    "  data: np.ndarray, \n",
    "  labels: np.ndarray, \n",
    "  test_size: float = 0.25, \n",
    "  random_state: int = None\n",
    "):\n",
    "  rng = np.random.default_rng(random_state)\n",
    "\n",
    "  num_samples = len(data)\n",
    "  num_test = int(test_size * num_samples)\n",
    "\n",
    "  indices = np.arange(num_samples)\n",
    "  rng.shuffle(indices)\n",
    "\n",
    "  test_indices = indices[:num_test]\n",
    "  train_indices = indices[num_test:]\n",
    "\n",
    "  X_train, X_test = data[train_indices], data[test_indices]\n",
    "  y_train, y_test = labels[train_indices], labels[test_indices]\n",
    "\n",
    "  return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def accuracy_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "  # Contar o número de predições corretas\n",
    "  correct_predictions = np.sum(y_true == y_pred)\n",
    "  # Calcular a acurácia\n",
    "  accuracy = correct_predictions / len(y_true)\n",
    "\n",
    "  return accuracy\n",
    "\n",
    "\n",
    "def confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "  # Calcular a matriz de confusão\n",
    "  unique_labels = np.unique(np.concatenate([y_true, y_pred]))\n",
    "  num_labels = len(unique_labels)\n",
    "  confusion_mat = np.zeros((num_labels, num_labels), dtype=int)\n",
    "\n",
    "  for true_label, pred_label in zip(y_true, y_pred):\n",
    "    true_idx = np.where(unique_labels == true_label)[0][0]\n",
    "    pred_idx = np.where(unique_labels == pred_label)[0][0]\n",
    "    confusion_mat[true_idx, pred_idx] += 1\n",
    "\n",
    "  return confusion_mat\n",
    "\n",
    "\n",
    "def test_3():\n",
    "  # Carregar o conjunto de dados Breast Cancer Wisconsin\n",
    "  data = load_breast_cancer()\n",
    "  X = data.data\n",
    "  y = data.target\n",
    "\n",
    "  # Dividir o conjunto de dados em treino e teste (80% treino, 20% teste)\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "  # Normalizar as features usando Z-Score\n",
    "  X_train_normalized, X_test_normalized = standard_scaler(X_train, X_test)\n",
    "\n",
    "  # Definir hiperparâmetros\n",
    "  layer_sizes = [X.shape[1], 10, 5, 1]\n",
    "  epochs = 10000\n",
    "  lr_0 = 0.1\n",
    "  batch_size = 64\n",
    "  lr = lr_0\n",
    "  reg_strength = 0.001\n",
    "\n",
    "  # Inicializar camadas com regularização\n",
    "  layers = [Layer(layer_sizes[i], layer_sizes[i + 1], reg_strength) for i in range(len(layer_sizes) - 1)]\n",
    "\n",
    "  train_sgd(layers, X_train_normalized, y_train, epochs, lr, batch_size, reg_strength)\n",
    "\n",
    "  # Testar o modelo no conjunto de teste\n",
    "  y_test_hat = forward(X_test_normalized, layers)\n",
    "  y_test_hat_binary = (y_test_hat > 0.5).astype(int).flatten()\n",
    "\n",
    "  # Calcular e imprimir a acurácia\n",
    "  accuracy = accuracy_score(y_test, y_test_hat_binary)\n",
    "  print(f\"\\nAcurácia: {accuracy:.4f}\")\n",
    "\n",
    "  # Calcular e imprimir a matriz de confusão\n",
    "  conf_matrix = confusion_matrix(y_test, y_test_hat_binary)\n",
    "  tn, fp, fn, tp = conf_matrix.ravel()\n",
    "  print(\"\\nMatriz de Confusão:\")\n",
    "  print(conf_matrix)\n",
    "  print('Verdadeiro Negativo:', tn)\n",
    "  print('Verdadeito Positivo:', tp)\n",
    "  print('Falso Positivo:', fp)\n",
    "  print('Falso Negativo:', fn)\n",
    "  \n",
    "test_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 4\n",
    "\n",
    "* Utilizar a técnica de k-fold cross-validation para selecionar os hiperparâmetros alpha (coeficiente da regularização L2) e learning rate. (2.0)\n",
    "* Plotar a variação da loss do dataset de validação vs cada hiperparâmetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizando busca pelo melhor conjunto de hiperparâmetros\n",
      "Hyperparams: reg strength: 0.001 | learning rate: 0.01\n",
      "Hyperparams: reg strength: 0.001 | learning rate: 0.1\n",
      "Hyperparams: reg strength: 0.01 | learning rate: 0.01\n",
      "Hyperparams: reg strength: 0.01 | learning rate: 0.1\n",
      "Hyperparams: reg strength: 0.1 | learning rate: 0.01\n",
      "Hyperparams: reg strength: 0.1 | learning rate: 0.1\n",
      "\n",
      "Resultados da otimização:\n",
      "1) Reg. Strength: 0.1, Learning Rate: 0.1, Mean Accuracy: 0.9681\n",
      "2) Reg. Strength: 0.01, Learning Rate: 0.1, Mean Accuracy: 0.9664\n",
      "3) Reg. Strength: 0.001, Learning Rate: 0.1, Mean Accuracy: 0.9628\n",
      "4) Reg. Strength: 0.01, Learning Rate: 0.01, Mean Accuracy: 0.7434\n",
      "5) Reg. Strength: 0.001, Learning Rate: 0.01, Mean Accuracy: 0.7221\n",
      "6) Reg. Strength: 0.1, Learning Rate: 0.01, Mean Accuracy: 0.7150\n",
      "\n",
      "O melhor conjunto de hiperparâmetros é: Reg. Strength: 0.1 e Learning Rate: 0.1\n"
     ]
    }
   ],
   "source": [
    "def k_fold(layers, X, y, k, epochs, learning_rate, batch_size, reg_strength, verbose=True):\n",
    "  fold_size = len(X) // k\n",
    "  accuracies = []\n",
    "  for i in range(k):\n",
    "    if verbose:\n",
    "      print(f'Fold {i+1} of {k}')\n",
    "    # Separar o conjunto em folds de treino e validação\n",
    "    val_start = i * fold_size\n",
    "    val_end = (i + 1) * fold_size\n",
    "    X_val_fold = X[val_start:val_end]\n",
    "    y_val_fold = y[val_start:val_end]\n",
    "    X_train_fold = np.concatenate([X[:val_start], X[val_end:]])\n",
    "    y_train_fold = np.concatenate([y[:val_start], y[val_end:]])\n",
    "\n",
    "    # Normalizar as features usando Z-Score\n",
    "    X_train_fold, X_val_fold = standard_scaler(X_train_fold, X_val_fold)\n",
    "\n",
    "    # Treinar o modelo MLP\n",
    "    train_sgd(layers, X_train_fold, y_train_fold, epochs, learning_rate, batch_size, reg_strength, verbose=False)\n",
    "\n",
    "    # Testar o modelo no conjunto de validação\n",
    "    y_val_hat = forward(X_val_fold, layers)\n",
    "    y_val_hat_binary = (y_val_hat > 0.5).astype(int).flatten()\n",
    "\n",
    "    # Calcular acurácia\n",
    "    accuracy = accuracy_score(y_val_fold, y_val_hat_binary)\n",
    "    accuracies.append(accuracy)\n",
    "  return accuracies\n",
    "\n",
    "\n",
    "def grid_search_l2_lr(layer_sizes, X, y, k, epochs, reg_strengths, learning_rates, batch_size):\n",
    "  results = []\n",
    "  for reg_strength in reg_strengths:\n",
    "    layers = [Layer(layer_sizes[i], layer_sizes[i + 1], reg_strength) for i in range(len(layer_sizes) - 1)]\n",
    "    for learning_rate in learning_rates:\n",
    "      print(f'Hyperparams: reg strength: {reg_strength} | learning rate: {learning_rate}')\n",
    "      acc = k_fold(layers, X, y, k, epochs, learning_rate, batch_size, reg_strength, verbose=False)\n",
    "      mean_acc = np.mean(acc)\n",
    "      results.append({\n",
    "        'reg_strength': reg_strength,\n",
    "        'learning_rate': learning_rate,\n",
    "        'mean_accuracy': mean_acc\n",
    "      })\n",
    "  return sorted(results, key=lambda x: x['mean_accuracy'], reverse=True)\n",
    "\n",
    "\n",
    "def test_4():\n",
    "  # Carregar o conjunto de dados Breast Cancer Wisconsin\n",
    "  data = load_breast_cancer()\n",
    "  X = data.data\n",
    "  y = data.target\n",
    "  k = 5  # Número de pastas\n",
    "  reg_strengths = [0.001, 0.01, 0.1]  # Coeficientes de regularização L2\n",
    "  learning_rates = [0.01, 0.1]  # Taxas de aprendizado\n",
    "  layer_sizes = [X.shape[1], 10, 5, 1]\n",
    "  epochs = 1000\n",
    "  batch_size = 64\n",
    "\n",
    "  print('Realizando busca pelo melhor conjunto de hiperparâmetros')\n",
    "  results = grid_search_l2_lr(layer_sizes, X, y, k, epochs, reg_strengths, learning_rates, batch_size)\n",
    "\n",
    "  # Imprimir os resultados\n",
    "  print(\"\\nResultados da otimização:\")\n",
    "  for i, result in enumerate(results):\n",
    "    print(f\"{i+1}) Reg. Strength: {result['reg_strength']}, Learning Rate: {result['learning_rate']}, Mean Accuracy: {result['mean_accuracy']:.4f}\")\n",
    "  \n",
    "  print(f'\\nO melhor conjunto de hiperparâmetros é: Reg. Strength: {results[0][\"reg_strength\"]} e Learning Rate: {results[0][\"learning_rate\"]}')\n",
    "\n",
    "test_4()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 5\n",
    "\n",
    "* Utilizar a técnica de k-fold cross-validation para selecionar o número de camadas escondidas e o número de neurônios em cada camada. (2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizando busca pelo melhor conjunto de hiperparâmetros\n",
      "Hyperparams: layers: 1 | units: 2\n",
      "Hyperparams: layers: 1 | units: 4\n",
      "Hyperparams: layers: 1 | units: 8\n",
      "Hyperparams: layers: 2 | units: 2\n",
      "Hyperparams: layers: 2 | units: 4\n",
      "Hyperparams: layers: 2 | units: 8\n",
      "Hyperparams: layers: 3 | units: 2\n",
      "Hyperparams: layers: 3 | units: 4\n",
      "Hyperparams: layers: 3 | units: 8\n",
      "\n",
      "Resultados da otimização:\n",
      "1) Layers: 1, Neurons: 8, Mean Accuracy: 0.9735\n",
      "2) Layers: 1, Neurons: 4, Mean Accuracy: 0.9699\n",
      "3) Layers: 1, Neurons: 2, Mean Accuracy: 0.9664\n",
      "4) Layers: 2, Neurons: 8, Mean Accuracy: 0.9575\n",
      "5) Layers: 2, Neurons: 4, Mean Accuracy: 0.9487\n",
      "6) Layers: 3, Neurons: 8, Mean Accuracy: 0.8991\n",
      "7) Layers: 3, Neurons: 4, Mean Accuracy: 0.7735\n",
      "8) Layers: 2, Neurons: 2, Mean Accuracy: 0.7699\n",
      "9) Layers: 3, Neurons: 2, Mean Accuracy: 0.6301\n",
      "\n",
      "O melhor conjunto de hiperparâmetros é: Layers: 1 e Neurons: 8\n"
     ]
    }
   ],
   "source": [
    "def grid_search_layers_units(layers, units, X, y, k, epochs, reg_strength, learning_rate, batch_size):\n",
    "  results = []\n",
    "  for n_layer in layers:\n",
    "    for n_units in units:\n",
    "      layer_sizes = [X.shape[1]] + [n_units] * n_layer + [1]\n",
    "      layers = [Layer(layer_sizes[i], layer_sizes[i + 1], reg_strength) for i in range(len(layer_sizes) - 1)]\n",
    "      print(f'Hyperparams: layers: {n_layer} | units: {n_units}')\n",
    "      acc = k_fold(layers, X, y, k, epochs, learning_rate, batch_size, reg_strength, verbose=False)\n",
    "      mean_acc = np.mean(acc)\n",
    "      results.append({\n",
    "        'n_layers': n_layer,\n",
    "        'n_units': n_units,\n",
    "        'mean_accuracy': mean_acc\n",
    "      })\n",
    "  return sorted(results, key=lambda x: x['mean_accuracy'], reverse=True)\n",
    "\n",
    "\n",
    "def test_5():\n",
    "  # Carregar o conjunto de dados Breast Cancer Wisconsin\n",
    "  data = load_breast_cancer()\n",
    "  X = data.data\n",
    "  y = data.target\n",
    "  k = 5  # Número de folds\n",
    "  reg_strength = 0.001\n",
    "  learning_rate = 0.1\n",
    "  epochs = 1000\n",
    "  batch_size = 64\n",
    "  layers = [1, 2, 3]\n",
    "  units = [2, 4, 8]\n",
    "\n",
    "  print('Realizando busca pelo melhor conjunto de hiperparâmetros')\n",
    "  results = grid_search_layers_units(layers, units, X, y, k, epochs, reg_strength, learning_rate, batch_size)\n",
    "\n",
    "  # Imprimir os resultados\n",
    "  print(\"\\nResultados da otimização:\")\n",
    "  for i, result in enumerate(results):\n",
    "    print(f\"{i+1}) Layers: {result['n_layers']}, Neurons: {result['n_units']}, Mean Accuracy: {result['mean_accuracy']:.4f}\")\n",
    "  \n",
    "  print(f'\\nO melhor conjunto de hiperparâmetros é: Layers: {results[0][\"n_layers\"]} e Neurons: {results[0][\"n_units\"]}')\n",
    "  \n",
    "test_5()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
